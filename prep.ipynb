{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis\n",
    "from joblib import load\n",
    "\n",
    "from scipy.signal import butter, filtfilt, find_peaks, peak_prominences\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daten laden**<br>\n",
    "Rohdaten werden aus einem Verzeichnis geladen und einem Dictionary data hinzugefügt.<br>\n",
    "Sollten Keine csv-Daten vorhanden sein bleibt das Dictionary data leer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden aller Daten aus dem Ordner: 'csv_data'\n",
    "def load_csv_data(folder_name):\n",
    "    data = {}\n",
    "    csv_folder = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "    if not os.path.exists(csv_folder):\n",
    "        print(f\"Error: The folder '{csv_folder}' was not found.\")\n",
    "        return data\n",
    "    \n",
    "    for file in os.listdir(csv_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(csv_folder, file)\n",
    "            try:\n",
    "                filename = file.replace('.csv', '')\n",
    "                data[filename] = pd.read_csv(file_path, delimiter=',')\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion load_csv_data wird mit dem Parameter des Verzeichnises in dem sich die csv-Daten befinden aufgerufen und der Variable data zugewiesen.<br>\n",
    "Danach wird versucht die jeweiligen Zeilen Gyroscope und Linear_Acceleration aus den jeweiligen Sprungdaten und Kniebeugendaten zu lesen.<br>\n",
    "Sollte einer der Schlüssel nicht gefunden werden wird ein KeyError geworfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data_train(folder_name):\n",
    "    data = load_csv_data(folder_name)\n",
    "    print(data.keys())\n",
    "\n",
    "    try:\n",
    "        jump_acc = data['Linear_Acceleration_Jump']\n",
    "        jump_gyr = data['Gyroscope_Jump']\n",
    "        squat_acc = data['Linear_Acceleration_Squat']\n",
    "        squat_gyr = data['Gyroscope_Squat']\n",
    "    except KeyError as e:\n",
    "        print(f\"Key not found: {e}\")\n",
    "\n",
    "    return jump_acc, jump_gyr, squat_acc, squat_gyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data_test(folder_name):\n",
    "    data = load_csv_data(folder_name)\n",
    "    print(data.keys())\n",
    "\n",
    "    try:\n",
    "        gyr = data['Gyroscope']\n",
    "        acc = data['Linear Acceleration']\n",
    "    except KeyError as e:\n",
    "        print(f\"Key not found: {e}\")\n",
    "\n",
    "    return acc, gyr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion trim_dataframes_to_shortest kürzt alle übergebenen Dataframes auf die Länge des kürzesten Dataframes.<br>\n",
    "Es wird die minimale Länge aller Dataframes bestimmt und anschließend werden alle Dataframes auf die minimale Länge gekürzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataframes_to_shortest(df_list):\n",
    "    # Bestimmen der minimalen Länge aller DataFrames in der Liste\n",
    "    min_length = min(df.shape[0] for df in df_list)\n",
    "\n",
    "    # Kürzen aller DataFrames auf die minimale Länge\n",
    "    trimmed_dfs = [df.iloc[:min_length] for df in df_list]\n",
    "\n",
    "    return trimmed_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion combine_relevant_columns erhält zwei Dataframes und erstellt ein neues Dataframe das nur bestimmte relevante Daten enthält.<br>\n",
    "Es werden die Spalten für die Lineare Beschleunigung auf der z und y Achse sowie die x Achse und die Zeit des Gyroscopes gelesen.<br>\n",
    "Am Ende wird ein neues Dataframe ausgegeben mit den gelesenen Spalten.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_relevant_columns(acc_df, gyr_df):\n",
    "    linear_acc_z = acc_df['Linear Acceleration z (m/s^2)']\n",
    "    linear_acc_y = acc_df['Linear Acceleration y (m/s^2)']\n",
    "\n",
    "    gyroskop_x = gyr_df['Gyroscope x (rad/s)']\n",
    "\n",
    "    time = gyr_df['Time (s)']\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Time (s)': time,\n",
    "        'Linear Acceleration z (m/s^2)': linear_acc_z,\n",
    "        'Linear Acceleration y (m/s^2)': linear_acc_y,\n",
    "        'Gyroscope x (rad/s)': gyroskop_x\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion filter_frequency führt einen Butterworth-Tiefpassfilter auf das mitgegebene Dataframe durch.<br>\n",
    "Es wird eine Kopie des übergebenen Dataframes erstellt um die gefilterten Daten zu speichern.<br>\n",
    "Die Filterordnung wird auf 5 festgelegt und der low_cutoff Frequenz wird auf 5Hz festgelegt.<br>\n",
    "Als nächstes wird der Butterworth Tiefpassfilter erstellt mit b und a als Koeffizienten.<br>\n",
    "Es wird über alle Spalten außer Time (s) mit einer Schleife iteriert und der Filter wird auf die jeweiligen Spalten angewendet und in das neue Kopierte Dataframe gespeichert.<br>\n",
    "Abschließend wird das Dataframe mit den neuen gefilterten Daten zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frequency(dataframe):\n",
    "    # Kopie des DataFrames erstellen\n",
    "    filtered_dataframe = dataframe.copy()\n",
    "\n",
    "    # Festlegen der Cutoff-Frequenz und der Filterordnung\n",
    "    filter_order = 5\n",
    "    low_cutoff = 0.3\n",
    "    high_cutoff = 10\n",
    "\n",
    "    # Butterworth Tiefpass erstellen\n",
    "    b, a = butter(filter_order, [low_cutoff,high_cutoff], btype='band', analog=False, fs=100)\n",
    "\n",
    "    # Schleife über alle Spalten außer 'Time (s)'\n",
    "    for column in dataframe.columns:\n",
    "        if column != 'Time (s)':\n",
    "            # Hochpassfilter auf die Signalreihe anwenden\n",
    "            filtered_signal = filtfilt(b, a, dataframe[column])\n",
    "            filtered_dataframe[column] = filtered_signal\n",
    "\n",
    "    return filtered_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code zur Normalisierung**<br>\n",
    "Um unsere Daten vorzubereiten, verwenden wir eine Normalisierungsfunktion, die die Merkmale 'Linear Acceleration z (m/s^2)',<br>\n",
    " 'Linear Acceleration y (m/s^2)' und 'Gyroscope x (rad/s)' skaliert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    # Kopie des DataFrames erstellen\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    columns_to_scale = ['Linear Acceleration z (m/s^2)', 'Linear Acceleration y (m/s^2)', 'Gyroscope x (rad/s)']\n",
    "\n",
    "    # Initialisieren des MinMaxScaler mit einem Bereich von -1 bis 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    # Fit des Scalers auf die Daten\n",
    "    scaler.fit(normalized_df[columns_to_scale])\n",
    "\n",
    "    # Transformieren der Daten\n",
    "    normalized_columns = scaler.transform(normalized_df[columns_to_scale])\n",
    "    \n",
    "    # Ersetzen der Originaldaten durch die normalisierten Daten\n",
    "    normalized_df[columns_to_scale] = normalized_columns\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Windowing**<br>\n",
    "Erstellen der Windows, mit Größe 100 und Overlap von 50. Der dynamische Windowing-Ansatz hat bei uns zu unakuraten<br> Ergebnissen\n",
    "im Entscheidungsbaum gesorgt, daher haben wir uns für die statische Variante entschieden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(df):\n",
    "    windows = []\n",
    "    window_size = 100\n",
    "    overlap = 50\n",
    "\n",
    "    for i in range(0, len(df), window_size - overlap):\n",
    "        window = df.iloc[i:i+window_size]\n",
    "        windows.append(window)\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labeling**\n",
    "Auf Basis der Windowgrößen werden  die Labels erstellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "    labels = []\n",
    "    window_size = 100\n",
    "    overlap = 50\n",
    "\n",
    "    for i in range(0, len(df), window_size - overlap):\n",
    "        window = df.iloc[i:i+window_size]\n",
    "        labels.append(window.iloc[0]['Label'])\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zusamenfügen der Dataframes**<br>\n",
    "Mit dem Addieren von `max_time_squat`auf die Zeit von`df2` wird eine kontinuierlicher Graph ohne Überscheidungen erzeugt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(df1, df2):\n",
    "    max_time_squat = df1['Time (s)'].max()\n",
    "    df2['Time (s)'] = df2['Time (s)'] + max_time_squat\n",
    "    return pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-Engineering**<br>\n",
    "Die Funktion `calculate_peak_features` extrahiert relevante statistische Daten über Peaks der y-Beschleunigungskomponente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_peak_features(window):\n",
    "    kurt = kurtosis(window['Linear Acceleration y (m/s^2)'])\n",
    " \n",
    "    peaks, _ = find_peaks(window['Linear Acceleration y (m/s^2)'])\n",
    "    num_peaks = len(peaks)\n",
    "    \n",
    "    prominences = peak_prominences(window['Linear Acceleration y (m/s^2)'], peaks)[0]\n",
    "    avg_prominence = np.mean(prominences) if prominences.size > 0 else 0\n",
    "    \n",
    "    return kurt, num_peaks, avg_prominence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden alle relevanten Features zum Training der Daten berechnet. Dazu werden die Daten zunächst in Fenster der \n",
    "Länge 100 mit einer Überlappung von 50 aufgeteilt. Für jedes Fenster werden dann die Features berechnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_df(windows):\n",
    "    feature_list = []\n",
    "\n",
    "    for window in windows:\n",
    "        acc_y = window['Linear Acceleration y (m/s^2)']\n",
    "        acc_z = window['Linear Acceleration z (m/s^2)']\n",
    "        gyr_x = window['Gyroscope x (rad/s)']\n",
    "        \n",
    "        gyr_x_var = gyr_x.var()\n",
    "        mean_acc_z = acc_z.mean()\n",
    "        mean_acc_y = acc_y.mean()\n",
    "        max_acc_y = acc_y.max()\n",
    "        quantile_75_acc_y = acc_y.quantile(0.75)\n",
    "        quartile_25_acc_y = acc_y.quantile(0.25)\n",
    "        std_acc_z = acc_z.std()\n",
    "        var_acc_z = acc_z.var()\n",
    "        min_acc_z = acc_z.min()\n",
    "        max_acc_z = acc_z.max()\n",
    "        range_acc_z = max_acc_z - min_acc_z\n",
    "        peak_acc_y = acc_y.max()\n",
    "        std_acc_y = acc_y.std()\n",
    "        var_acc_y = acc_y.var()\n",
    "        min_acc_y = acc_y.min()\n",
    "        range_acc_y = peak_acc_y - min_acc_y\n",
    "        range_gyr_x = gyr_x.max() - gyr_x.min()\n",
    "        range_acc_z = acc_z.max() - acc_z.min()\n",
    "        min_acc_y = acc_y.min()\n",
    "\n",
    "\n",
    "        kurt, num_peaks, avg_prominence = calculate_peak_features(window)\n",
    "\n",
    "\n",
    "        features = {\n",
    "            'gyr_x_var': gyr_x_var,\n",
    "            'avg_prominence': avg_prominence,\n",
    "            'kurt' : kurt,\n",
    "            'num_peaks' : num_peaks,\n",
    "            'min_acc_y': min_acc_y,\n",
    "            'mean_acc_z': mean_acc_z,\n",
    "            'quantile_75_acc_y': quantile_75_acc_y,\n",
    "            'quantile_25_acc_y': quartile_25_acc_y,\n",
    "            'peak_acc_y': max_acc_y,\n",
    "            'min_acc_z': min_acc_z,\n",
    "            'peak_acc_z': max_acc_z,\n",
    "            'range_gyr_x': range_gyr_x,\n",
    "            'range_acc_z': range_acc_z,\n",
    "            'var_acc_z': var_acc_z,\n",
    "            'std_acc_z': std_acc_z,\n",
    "            'mean_acc_y': mean_acc_y,\n",
    "            'std_acc_y': std_acc_y,\n",
    "            'var_acc_y': var_acc_y,\n",
    "            'range_acc_y': range_acc_y,\n",
    "            'min_acc_y': min_acc_y\n",
    "        }\n",
    "        feature_list.append(features)\n",
    "\n",
    "    return pd.DataFrame(feature_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pfad):\n",
    "    try:\n",
    "        modell = load(pfad)\n",
    "        print(f\"Modell erfolgreich von '{pfad}' geladen.\")\n",
    "        return modell\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Modell konnte nicht gefunden werden: '{pfad}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten beim Laden des Modells: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
